{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:40:27.671240Z",
     "start_time": "2021-01-25T19:40:25.760279Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:40:30.495968Z",
     "start_time": "2021-01-25T19:40:27.675204Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/out/data.pkl')\n",
    "df['year'] = df.date.dt.year.astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:40:30.514514Z",
     "start_time": "2021-01-25T19:40:30.497845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1893     1\n",
       "1894     2\n",
       "1895     9\n",
       "1896     6\n",
       "1897     4\n",
       "1898     7\n",
       "1899     1\n",
       "1900     6\n",
       "1901     2\n",
       "1902     3\n",
       "1903     4\n",
       "1904     1\n",
       "1905     3\n",
       "1906     1\n",
       "1907     8\n",
       "1908     6\n",
       "1909     5\n",
       "1910    12\n",
       "1911    42\n",
       "1912    32\n",
       "1913    37\n",
       "1914    50\n",
       "1915    50\n",
       "1916    60\n",
       "1917    44\n",
       "1918    53\n",
       "1919    48\n",
       "1920    53\n",
       "1921    60\n",
       "1922    55\n",
       "1923    58\n",
       "1924    84\n",
       "1925    90\n",
       "1926    77\n",
       "1927    95\n",
       "2013    70\n",
       "2014     4\n",
       "Name: year, dtype: Int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Years with less than 100 movies\n",
    "year_movie_count = df.year.value_counts()\n",
    "year_movie_count[year_movie_count < 100].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf-idf with sublinear tf scaling for a term $t$ of a document $d$ is given by\n",
    "\n",
    "$$\\text{tf-idf}(t, d) = (1 + \\log\\text{tf}(t, d)) \\cdot \\left(\\log\\frac{1 + n}{\\text{1 + df}(t)} + 1\\right)$$\n",
    "\n",
    "where $n$ is the total number of documents and $\\text{df}(t)$ is the document frequency of $t$ ([source](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)).\n",
    "\n",
    "We use sublinear tf scaling to reduce the significance of very common terms which are likely uninformative.\n",
    "\n",
    "Here, a term is a phrase and a document is a year. Given a tf-idf vector for each year, we then normalize each tf-idf vector to have a Euclidean norm of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:40:39.035840Z",
     "start_time": "2021-01-25T19:40:30.521313Z"
    }
   },
   "outputs": [],
   "source": [
    "phrases_by_year = df.query('1928 < date.dt.year <= 2012').groupby('year').phrases.agg(lambda x: sum(x, []))\n",
    "phrase_counts = pd.DataFrame(phrases_by_year.apply(Counter).tolist(), index=phrases_by_year.index).fillna(0).sort_index(1)\n",
    "phrase_counts = phrase_counts.loc[:, phrase_counts.sum().ge(10)]\n",
    "phrases_tfidf = pd.DataFrame(TfidfTransformer(sublinear_tf=True).fit_transform(phrase_counts).toarray(), index=phrase_counts.index, columns=phrase_counts.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:40:39.124916Z",
     "start_time": "2021-01-25T19:40:39.039335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "1929    [stan and ollie, laurel and hardy, minnie mous...\n",
       "1930    [stan and ollie, robert montgomery, laurel and...\n",
       "1931    [betty boop, bimbo, stymie, speakeasy, stan an...\n",
       "1932    [bimbo, stymie, betty boop, spanky, stan and o...\n",
       "1933    [bimbo, betty boop, stymie, spanky, oliver hardy]\n",
       "                              ...                        \n",
       "2008    [heart attack, internet, chandigarh, text mess...\n",
       "2009    [heart attack, internet, black market, global ...\n",
       "2010    [heart attack, fashion designer, internet, int...\n",
       "2011    [heart attack, post-credits scene, santhanam, ...\n",
       "2012    [heart attack, viral, facebook, software engin...\n",
       "Length: 84, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_tfidf.apply(lambda x: x.nlargest().index.values, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
