{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:23:22.236430Z",
     "start_time": "2021-01-25T19:23:20.573235Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:23:24.569479Z",
     "start_time": "2021-01-25T19:23:22.238630Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/out/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:23:24.589527Z",
     "start_time": "2021-01-25T19:23:24.575222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1893    1\n",
       "1894    2\n",
       "1895    9\n",
       "1896    6\n",
       "1897    4\n",
       "1898    7\n",
       "1899    1\n",
       "1900    6\n",
       "1901    2\n",
       "1902    3\n",
       "1903    4\n",
       "1904    1\n",
       "1905    3\n",
       "1906    1\n",
       "1907    8\n",
       "1908    6\n",
       "1909    5\n",
       "2014    4\n",
       "Name: date, dtype: Int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Years with less than 10 movies\n",
    "year_movie_count = df.date.dt.year.astype('Int64').value_counts()\n",
    "year_movie_count[year_movie_count < 10].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf-idf with sublinear tf scaling for a term $t$ of a document $d$ is given by\n",
    "\n",
    "$$\\text{tf-idf}(t, d) = (1 + \\log\\text{tf}(t, d)) \\cdot \\left(\\log\\frac{1 + n}{\\text{1 + df}(t)} + 1\\right)$$\n",
    "\n",
    "where $n$ is the total number of documents and $\\text{df}(t)$ is the document frequency of $t$ ([source](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)).\n",
    "\n",
    "We use sublinear tf scaling to reduce the significance of very common terms which are likely uninformative.\n",
    "\n",
    "Here, a term is a phrase and a document is a year. Given a tf-idf vector for each year, we then normalize each tf-idf vector to have a Euclidean norm of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:23:34.102581Z",
     "start_time": "2021-01-25T19:23:24.591587Z"
    }
   },
   "outputs": [],
   "source": [
    "phrases_by_year = df.query('1909 < date.dt.year < 2014').groupby(df.date.dt.year.astype('Int64')).phrases.agg(lambda x: sum(x, []))\n",
    "phrase_counts = pd.DataFrame(phrases_by_year.apply(Counter).tolist(), index=phrases_by_year.index).fillna(0).sort_index(1)\n",
    "phrase_counts = phrase_counts.loc[:, phrase_counts.sum().ge(10)]\n",
    "phrases_tfidf = pd.DataFrame(TfidfTransformer(sublinear_tf=True).fit_transform(phrase_counts).toarray(), index=phrase_counts.index, columns=phrase_counts.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T19:23:34.157240Z",
     "start_time": "2021-01-25T19:23:34.104526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "1910    [bushranger, aboriginal, ebenezer scrooge, rac...\n",
       "1911    [bushranger, oxford university, feature film, ...\n",
       "1912    [national library, oxford university, feature ...\n",
       "1913    [oxford university, feature film, new south wa...\n",
       "1914    [chaplin, charlie chaplin, ambrose, illegitima...\n",
       "                              ...                        \n",
       "2009    [heart attack, internet, black market, video, ...\n",
       "2010    [heart attack, internet, fashion designer, int...\n",
       "2011    [heart attack, post-credits scene, santhanam, ...\n",
       "2012    [heart attack, viral, facebook, software engin...\n",
       "2013    [film, smithsonian, true life, upper-class, foal]\n",
       "Length: 104, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_tfidf.apply(lambda x: x.nlargest().index.values, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
